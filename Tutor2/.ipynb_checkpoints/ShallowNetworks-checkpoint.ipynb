{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_true:  tensor([ 5.7558,  7.1344, -0.8940])\n",
      "b_true:  tensor([2.0407])\n",
      "y_true:  tensor([  7.2374,   6.3132, -12.3726,  -3.9817,   6.1765,   0.9688,  10.9805,\n",
      "         -7.5871,  12.1560,   9.2296])\n"
     ]
    }
   ],
   "source": [
    "### Problem 1\n",
    "x = torch.randn((10, 3))\n",
    "\n",
    "w_true =  5 * torch.randn(3)\n",
    "b_true = -2 * torch.randn(1)\n",
    "y_true = torch.matmul(x, w_true) + b_true\n",
    "print (\"w_true: \", w_true)\n",
    "print (\"b_true: \", b_true)\n",
    "print (\"y_true: \", y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_pred:  tensor([-1.2404,  1.0884, -0.4867], requires_grad=True)\n",
      "b_pred:  tensor([-0.7475], requires_grad=True)\n",
      "y_pred:  tensor([-0.5542, -2.0652, -1.1753,  0.7891, -0.3401, -0.3962, -1.5127, -2.2724,\n",
      "        -1.4876, -3.4370], grad_fn=<AddBackward0>)\n",
      "requires_grad:  True\n"
     ]
    }
   ],
   "source": [
    "### Problem 2\n",
    "w_pred = torch.randn(3, requires_grad = True)\n",
    "b_pred = torch.randn(1, requires_grad = True)\n",
    "y_pred = torch.matmul(x, w_pred) + b_pred\n",
    "\n",
    "print (\"w_pred: \", w_pred)\n",
    "print (\"b_pred: \", b_pred)\n",
    "print (\"y_pred: \", y_pred)\n",
    "print (\"requires_grad: \", y_pred.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 3\n",
    "def loss_function(w, b):\t\t# Mean Square Error\n",
    "\ty_pred = torch.matmul(x, w) + b\n",
    "\treturn torch.sum(torch.pow(y_pred - y_true, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad:  True\n",
      "\n",
      "1st round\n",
      "loss:  tensor(854.2935, grad_fn=<SumBackward0>)\n",
      "w_grad:  tensor([-139.8670,  -86.5375,  -61.2844])\n",
      "b_grad:  tensor([-83.1444])\n",
      "\n",
      "2nd round\n",
      "loss:  tensor(854.2935, grad_fn=<SumBackward0>)\n",
      "w_grad:  tensor([-279.7339, -173.0751, -122.5689])\n",
      "b_grad:  tensor([-166.2888])\n",
      "\n",
      "Initialization\n",
      "w_grad:  tensor([0., 0., 0.])\n",
      "b_grad:  tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "### Problem 4\n",
    "loss = loss_function(w_pred, b_pred)\n",
    "print (\"requires_grad: \", loss.requires_grad)\n",
    "\n",
    "print (\"\\n1st round\")\n",
    "loss = loss_function(w_pred, b_pred)\n",
    "loss.backward()\n",
    "print (\"loss: \", loss)\n",
    "print (\"w_grad: \", w_pred.grad)\n",
    "print (\"b_grad: \", b_pred.grad)\n",
    "\n",
    "print (\"\\n2nd round\")\n",
    "loss = loss_function(w_pred, b_pred)\n",
    "loss.backward()\n",
    "print (\"loss: \", loss)\n",
    "print (\"w_grad: \", w_pred.grad)\n",
    "print (\"b_grad: \", b_pred.grad)\n",
    "\n",
    "print (\"\\nInitialization\")\n",
    "w_pred.grad.data.zero_()\n",
    "b_pred.grad.data.zero_()\n",
    "print (\"w_grad: \", w_pred.grad)\n",
    "print (\"b_grad: \", b_pred.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 010th: loss 44.05799865722656\n",
      "Round 020th: loss 6.293754577636719\n",
      "Round 030th: loss 1.4968458414077759\n",
      "Round 040th: loss 0.4089859426021576\n",
      "Round 050th: loss 0.1147087961435318\n",
      "Round 060th: loss 0.03231625258922577\n",
      "Round 070th: loss 0.009111170656979084\n",
      "Round 080th: loss 0.0025691529735922813\n",
      "Round 090th: loss 0.000724425190128386\n",
      "Round 100th: loss 0.00020424340618774295\n",
      "Round 110th: loss 5.760488420492038e-05\n",
      "Round 120th: loss 1.623987191123888e-05\n",
      "\n",
      "Total 124 iterations.\n",
      "w_pred:  tensor([ 5.7563,  7.1331, -0.8932], requires_grad=True)\n",
      "b_pred:  tensor([2.0401], requires_grad=True)\n",
      "y_pred:  tensor([  7.2368,   6.3130, -12.3726,  -3.9827,   6.1754,   0.9681,  10.9803,\n",
      "         -7.5855,  12.1551,   9.2309], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Problem 5 - Linear Regression\n",
    "w_pred = torch.randn(3, requires_grad = True)\n",
    "b_pred = torch.randn(1, requires_grad = True)\n",
    "loss = torch.tensor([1])\n",
    "\n",
    "k = 0\n",
    "alpha = 0.01\n",
    "while loss > 1e-5:\n",
    "\tk = k + 1\n",
    "\tloss = loss_function(w_pred, b_pred)\n",
    "\tloss.backward()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tw_pred -= alpha * w_pred.grad\n",
    "\t\tb_pred -= alpha * b_pred.grad\n",
    "\n",
    "\tw_pred.grad.data.zero_()\n",
    "\tb_pred.grad.data.zero_()\n",
    "\tif k % 10 == 0: print (\"Round {:03d}th: loss {}\".format(k, loss))\n",
    "\n",
    "y_pred = torch.matmul(x, w_pred) + b_pred\n",
    "print(\"\\nTotal {} iterations.\".format(k))\n",
    "print (\"w_pred: \", w_pred)\n",
    "print (\"b_pred: \", b_pred)\n",
    "print (\"y_pred: \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_true:  tensor([-1.4874, -0.3020, -3.1120])\n",
      "b_true:  tensor([-1.4896])\n",
      "y_true:  tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Round 01000th: loss 0.1644587218761444\n",
      "Round 02000th: loss 0.08585626631975174\n",
      "Round 03000th: loss 0.058352552354335785\n",
      "\n",
      "Total 3525 iterations.\n",
      "w_pred:  tensor([-2.2627, -2.0985, -2.5832], requires_grad=True)\n",
      "b_pred:  tensor([-4.8164], requires_grad=True)\n",
      "y_pred:  tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "### Problem 5 - Logistic Regression\n",
    "w_true =  5 * torch.randn(3)\n",
    "b_true = -2 * torch.randn(1)\n",
    "y_true = torch.matmul(x, w_true) + b_true\n",
    "y_true = (y_true > 0).type(torch.FloatTensor)\n",
    "print (\"w_true: \", w_true)\n",
    "print (\"b_true: \", b_true)\n",
    "print (\"y_true: \", y_true)\n",
    "\n",
    "def loss_function(w, b):\t\t# Cross Entropy\n",
    "\ty_pred = torch.matmul(x, w) + b\n",
    "\ty_pred = torch.exp(y_pred) / (1 + torch.exp(y_pred))\n",
    "\treturn -torch.sum(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "\n",
    "w_pred = torch.randn(3, requires_grad = True)\n",
    "b_pred = torch.randn(1, requires_grad = True)\n",
    "loss = torch.tensor([1])\n",
    "\n",
    "k = 0\n",
    "alpha = 0.01\n",
    "while torch.abs(loss) > 0.05 > 1e-5:\n",
    "\tk = k + 1\n",
    "\tloss = loss_function(w_pred, b_pred)\n",
    "\tloss.backward()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tw_pred -= alpha * w_pred.grad\n",
    "\t\tb_pred -= alpha * b_pred.grad\n",
    "\n",
    "\tw_pred.grad.data.zero_()\n",
    "\tb_pred.grad.data.zero_()\n",
    "\tif k % 1000 == 0: print (\"Round {:05d}th: loss {}\".format(k, loss))\n",
    "\n",
    "y_pred = torch.matmul(x, w_pred) + b_pred\n",
    "y_pred = (y_pred > 0).type(torch.FloatTensor)\n",
    "print(\"\\nTotal {} iterations.\".format(k))\n",
    "print (\"w_pred: \", w_pred)\n",
    "print (\"b_pred: \", b_pred)\n",
    "print (\"y_pred: \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
